#!/usr/bin/env python

"""Evaluate a given classifier against a directory of annotated images.
The script will show some metric about precision/recall, average precision etc. """

import argparse
import pprint
import csv
import itertools

import numpy as np

import matplotlib.pyplot as plt
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import average_precision_score
from sklearn.metrics import roc_auc_score, roc_curve, auc, confusion_matrix
from scipy import interp
from itertools import cycle

def y_score_from_predictions(ordered_classes, predictions_per_image):
    """
    Create a dense array of the score per class for each image

    :param ordered_classes: the labels used in the classification. These are assumed to be in the same order as passed to other functions
    :param predictions_per_image: a sequence of [score0, ..., scoreN] sequences, one for each image. Each score-sequence must have the same  order as the ordered_classes
    :return: numpy.ndarray of shape (number of images, number of classes)
    Axis 0 indexes over images, axis 1 is the score for the class at that position
    y_score[x][y] is the score of image x for the class ordered_classes[y]
    """
    n_images = len(predictions_per_image)
    n_classes = len(ordered_classes)

    y_score = np.zeros(shape=(n_images, n_classes))

    for image_index, prediction_per_image in enumerate(predictions_per_image):
        prediction_dict = dict(zip(ordered_classes, prediction_per_image))
        for label_index, label in enumerate(ordered_classes):
            y_score[image_index][label_index] = prediction_dict[label]
    return y_score


def y_test_from_ground_truth(ordered_classes, ground_truth_per_image):
    """
    Create a sparse array of the one-hot ground truth classification per image

    :param ordered_classes: the labels used in the classification. These are assumed to be in the same order as passed to other functions
    :param ground_truth_per_image: a list of classes, one for each image
    :return: numpy.ndarray of shape (number of images, number of classes)
    Axis 0 indexes over images, axis 1 is the score for the class at that position
    (i.e. 1 because this takes the ground truth as input)
    y_test[x][y] is 1 when the ground truth class for image x is ordered_classes[y]. The other values are 0
    """
    n_images = len(predictions_per_image)
    n_classes = len(ordered_classes)

    y_test = np.zeros(shape=(n_images, n_classes))
    for image_index, ground_truth_class in enumerate(ground_truth_per_image):
        y_test[image_index][ordered_classes.index(ground_truth_class)] = 1  # Others are left at zero

    return y_test

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    #     print("Normalized confusion matrix")
    # else:
    #     print('Confusion matrix, without normalization')
    #
    # print(cm)

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=80)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

parser = argparse.ArgumentParser(description='Calculate various metrics of a classifier that assigns a score per class')

parser.add_argument('-i', '--input', type=str, help='Input csv file. First column is ground truth label, other columns are the per-class scores, as output by `evaluate_classifier`',
                    required=True,
                    default="result.csv")

args = parser.parse_args()

ordered_classes = []

labels_per_image = []
predictions_per_image = []

if args.input:
    with open(args.input, 'r') as f:
        reader = csv.reader(f)

        header_row = reader.next()
        ordered_classes = header_row[1:]  # First column is the ground truth

        for row in reader:
            labels_per_image += [row[0]]
            predictions_per_image += [row[1:]]

n_images = len(labels_per_image)
print "Read {} prediction scores".format(n_images)


n_classes = len(ordered_classes)
y_score = y_score_from_predictions(ordered_classes, predictions_per_image)
y_test = y_test_from_ground_truth(ordered_classes, labels_per_image)

y_class_indices = y_score.argmax(axis=1)  # Find the index for each row that has the maximum value
y_classes = [ordered_classes[class_index] for class_index in y_class_indices]

# setup consistent colors for each class
# colors = cycle(['navy', 'turquoise', 'darkorange', 'cornflowerblue', 'teal'])
cmap = plt.get_cmap('gist_rainbow')
colors = [cmap(i) for i in np.linspace(0, 1, n_classes)]

class_colors = dict(zip(ordered_classes, colors))

# For each class
precision = dict()
recall = dict()
average_precision = dict()

for i in range(n_classes):
    precision[i], recall[i], _ = precision_recall_curve(y_test[:, i],
                                                        y_score[:, i])
    average_precision[i] = average_precision_score(y_test[:, i], y_score[:, i])

# A "micro-average": quantifying score on all classes jointly
precision["micro"], recall["micro"], _ = precision_recall_curve(y_test.ravel(),
                                                                y_score.ravel())
average_precision["micro"] = average_precision_score(y_test, y_score,
                                                     average="micro")
print('Average precision score, micro-averaged over all classes: {0:0.2f}'
      .format(average_precision["micro"]))

plt.figure()
plt.step(recall['micro'], precision['micro'], color='b', alpha=0.2,
         where='post')
plt.fill_between(recall["micro"], precision["micro"], step='post', alpha=0.2,
                 color='b')

plt.xlabel('Recall')
plt.ylabel('Precision')
plt.ylim([0.0, 1.05])
plt.xlim([0.0, 1.0])
plt.title(
    'Average precision score, micro-averaged over all classes: AP={0:0.2f}'
        .format(average_precision["micro"]))


plt.figure(figsize=(7, 8))
f_scores = np.linspace(0.2, 0.8, num=4)
lines = []
labels = []
for f_score in f_scores:
    x = np.linspace(0.01, 1)
    y = f_score * x / (2 * x - f_score)
    l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)
    plt.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))

lines.append(l)
labels.append('iso-f1 curves')
l, = plt.plot(recall["micro"], precision["micro"], color='gold', lw=2)
lines.append(l)
labels.append('micro-average Precision-recall (area = {0:0.2f})'
              ''.format(average_precision["micro"]))

for i in range(n_classes):
    cls = ordered_classes[i]
    l, = plt.plot(recall[i], precision[i], lw=2, color=class_colors[cls])
    lines.append(l)
    labels.append('Precision-recall for class "{0}" (area = {1:0.2f})'
                  ''.format(cls, average_precision[i]))

fig = plt.gcf()
fig.subplots_adjust(bottom=0.25)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Extension of Precision-Recall curve to multi-class')
plt.legend(lines, labels, loc=(0, -.38), prop=dict(size=14))


# plt.figure(figsize=(7, 8))
# Compute ROC curve and ROC area for each class
fpr = dict()
tpr = dict()
thresholds = dict()
roc_auc = dict()
for i in range(n_classes):
    fpr[i], tpr[i], thresholds[i] = roc_curve(y_test[:, i], y_score[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])
#
# Compute micro-average ROC curve and ROC area
fpr["micro"], tpr["micro"], _ = roc_curve(y_test.ravel(), y_score.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

lw = 2
# plt.plot(fpr[2], tpr[2], color='darkorange',
#          lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2])
# plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
# plt.xlim([0.0, 1.0])
# plt.ylim([0.0, 1.05])
# plt.xlabel('False Positive Rate')
# plt.ylabel('True Positive Rate')
# plt.title('Receiver operating characteristic example')
# plt.legend(loc="lower right")


# Compute macro-average ROC curve and ROC area

# First aggregate all false positive rates
all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))

# Then interpolate all ROC curves at this points
mean_tpr = np.zeros_like(all_fpr)
for i in range(n_classes):
    mean_tpr += interp(all_fpr, fpr[i], tpr[i])

# Finally average it and compute AUC
mean_tpr /= n_classes

fpr["macro"] = all_fpr
tpr["macro"] = mean_tpr
roc_auc["macro"] = auc(fpr["macro"], tpr["macro"])

# Plot all ROC curves
plt.figure(figsize=(7, 8))
plt.plot(fpr["micro"], tpr["micro"],
         label='micro-average ROC curve (area = {0:0.2f})'
               ''.format(roc_auc["micro"]),
         color='deeppink', linestyle=':', linewidth=4)

plt.plot(fpr["macro"], tpr["macro"],
         label='macro-average ROC curve (area = {0:0.2f})'
               ''.format(roc_auc["macro"]),
         color='navy', linestyle=':', linewidth=4)

for i in range(n_classes):
    cls = ordered_classes[i]
    plt.plot(fpr[i], tpr[i], lw=lw, color=class_colors[cls],
             label='ROC curve of class {0} (area = {1:0.2f})'
             ''.format(cls, roc_auc[i]))

plt.plot([0, 1], [0, 1], 'k--', lw=lw)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Some extension of Receiver operating characteristic to multi-class')
plt.legend(loc="lower right")

# Plot threshold vs true and false positive rate
plt.figure()

for i in range(n_classes):
    cls = ordered_classes[i]
    plt.plot(thresholds[i],
             tpr[i],
             color=class_colors[cls],
             lw=lw,
             label='Tpr {}'.format(ordered_classes[i]))

    plt.plot(thresholds[i],
             fpr[i],
             color=class_colors[cls],
             lw=lw,
             linestyle='dashed',
             label='Fpr {}'.format(ordered_classes[i]))
plt.ylabel('True & False Positive Rate')
plt.xlabel('Threshold')
plt.title('Threshold vs True & False positive rate')
plt.legend(loc="lower right")

# Compute confusion matrix
cnf_matrix = confusion_matrix(labels_per_image, y_classes)
# np.set_printoptions(precision=2)

# # Plot non-normalized confusion matrix
# plt.figure()
# plot_confusion_matrix(cnf_matrix, classes=ordered_classes,
#                       title='Confusion matrix, without normalization')

# Plot normalized confusion matrix
plt.figure()
plot_confusion_matrix(cnf_matrix, classes=ordered_classes, normalize=True,
                      title='Normalized confusion matrix')

plt.show()

