#!/usr/bin/env python

"""Evaluate a given classifier against a directory of annotated images.
The script will show some metric about precision/recall, average precision etc. """

import argparse
import pprint
import csv

import numpy as np

import matplotlib.pyplot as plt
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import average_precision_score
from itertools import cycle

def y_score_from_predictions(ordered_classes, predictions_per_image):
    """
    Create a dense array of the score per class for each image

    :param ordered_classes: the labels used in the classification. These are assumed to be in the same order as passed to other functions
    :param predictions_per_image: a sequence of [score0, ..., scoreN] sequences, one for each image. Each score-sequence must have the same  order as the ordered_classes
    :return: numpy.ndarray of shape (number of images, number of classes)
    Axis 0 indexes over images, axis 1 is the score for the class at that position
    y_score[x][y] is the score of image x for the class ordered_classes[y]
    """
    n_images = len(predictions_per_image)
    n_classes = len(ordered_classes)

    y_score = np.zeros(shape=(n_images, n_classes))
    for image_index, prediction_per_image in enumerate(predictions_per_image):
        prediction_dict = dict(zip(ordered_classes, prediction_per_image))
        for label_index, label in enumerate(ordered_classes):
            y_score[image_index][label_index] = prediction_dict[label]
    return y_score

def y_test_from_ground_truth(ordered_classes, ground_truth_per_imsage):
    """
    Create a sparse array of the one-hot ground truth classification per image

    :param ordered_classes: the labels used in the classification. These are assumed to be in the same order as passed to other functions
    :param ground_truth_per_image: a list of classes, one for each image
    :return: numpy.ndarray of shape (number of images, number of classes)
    Axis 0 indexes over images, axis 1 is the score for the class at that position
    (i.e. 1 because this takes the ground truth as input)
    y_test[x][y] is 1 when the ground truth class for image x is ordered_classes[y]. The other values are 0
    """
    n_images = len(predictions_per_image)
    n_classes = len(ordered_classes)

    y_test = np.zeros(shape=(n_images, n_classes))
    for image_index, ground_truth_class in enumerate(ground_truth_per_image):
        y_test[image_index][ordered_classes.index(ground_truth_class)] = 1  # Others are left at zero

    return y_test

parser = argparse.ArgumentParser(description='Calculate various metrics of a classifier that assigns a score per class')

parser.add_argument('-i', '--input', type=str, help='Input csv file. First column is ground truth label, other columns are the per-class scores, as output by `evaluate_classifier`',
                    required=True,
                    default="result.csv")

args = parser.parse_args()

ordered_classes = []

labels_per_image = []
predictions_per_image = []

if args.input:
    with open(args.input, 'r') as f:
        reader = csv.reader(f)

        header_row = reader.next()
        ordered_classes = header_row[1:]  # First column is the ground truth

        for row in reader:
            labels_per_image += [row[0]]
            predictions_per_image += [row[1:]]

n_images = len(labels_per_image)
print "Read {} prediction scores".format(n_images)

import ipdb; ipdb.set_trace()

n_classes = len(ordered_classes)
y_score = y_score_from_predictions(ordered_classes, predictions_per_image)
Y_test = y_test_from_ground_truth(ordered_classes, labels_per_image)

# For each class
precision = dict()
recall = dict()
average_precision = dict()

for i in range(n_classes):
    precision[i], recall[i], _ = precision_recall_curve(Y_test[:, i],
                                                        y_score[:, i])
    average_precision[i] = average_precision_score(Y_test[:, i], y_score[:, i])

# A "micro-average": quantifying score on all classes jointly
precision["micro"], recall["micro"], _ = precision_recall_curve(Y_test.ravel(),
                                                                y_score.ravel())
average_precision["micro"] = average_precision_score(Y_test, y_score,
                                                     average="micro")
print('Average precision score, micro-averaged over all classes: {0:0.2f}'
      .format(average_precision["micro"]))

plt.figure()
plt.step(recall['micro'], precision['micro'], color='b', alpha=0.2,
         where='post')
plt.fill_between(recall["micro"], precision["micro"], step='post', alpha=0.2,
                 color='b')

plt.xlabel('Recall')
plt.ylabel('Precision')
plt.ylim([0.0, 1.05])
plt.xlim([0.0, 1.0])
plt.title(
    'Average precision score, micro-averaged over all classes: AP={0:0.2f}'
        .format(average_precision["micro"]))

# setup plot details
colors = cycle(['navy', 'turquoise', 'darkorange', 'cornflowerblue', 'teal'])

plt.figure(figsize=(7, 8))
f_scores = np.linspace(0.2, 0.8, num=4)
lines = []
labels = []
for f_score in f_scores:
    x = np.linspace(0.01, 1)
    y = f_score * x / (2 * x - f_score)
    l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)
    plt.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))

lines.append(l)
labels.append('iso-f1 curves')
l, = plt.plot(recall["micro"], precision["micro"], color='gold', lw=2)
lines.append(l)
labels.append('micro-average Precision-recall (area = {0:0.2f})'
              ''.format(average_precision["micro"]))

for i, color in zip(range(n_classes), colors):
    l, = plt.plot(recall[i], precision[i], lw=2) # , color=color
    lines.append(l)
    labels.append('Precision-recall for class "{0}" (area = {1:0.2f})'
                  ''.format(ordered_classes[i], average_precision[i]))

fig = plt.gcf()
fig.subplots_adjust(bottom=0.25)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Extension of Precision-Recall curve to multi-class')
plt.legend(lines, labels, loc=(0, -.38), prop=dict(size=14))

plt.show()

