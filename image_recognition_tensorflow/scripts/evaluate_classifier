#!/usr/bin/env python

"""Evaluate a given classifier against a directory of annotated images.
The script will output a .csv that can be evaluated. """

import argparse

from image_recognition_tensorflow.evaluate_classifier import evaluate_classifier

# Assign description to the help doc
parser = argparse.ArgumentParser(description='Evaluate the classification performance of a TensorFlow network')

# Add arguments
parser.add_argument('graph_path', type=str, help='Path to a trained TensorFlow output_graph.pb network')
parser.add_argument('labels_path', type=str, help='Path to a file with the output labels, eg. output_labels.txt')
parser.add_argument('annotated_dir', type=str, help='Use a the given directory (which must have subdirs for each class)'
                                                    ' and use the annotations to get some metrics about the classifier')
parser.add_argument('-n', '--top_n', type=int, help='Top N results to display', default=1)
parser.add_argument('--input-tensor', type=str, help='Tensor name of input image', default="Cast:0")
parser.add_argument('--output-tensor', type=str, help='Tensor name of classifications label', default="final_result:0")
parser.add_argument('-o', '--output', type=str, help='Classification result output csv. First column is ground truth '
                                                     'label, other columns are the class scores', default="result.csv")
args = parser.parse_args()
evaluate_classifier(args.graph_path, args.labels_path, args.annotated_dir, args.output, args.input_tensor,
                    args.output_tensor)
