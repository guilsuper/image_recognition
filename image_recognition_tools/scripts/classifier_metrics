#!/usr/bin/env python

"""Evaluate a given classifier against a directory of annotated images.
The script will show some metric about precision/recall, average precision etc. """

import argparse
import itertools

import numpy as np

import matplotlib.pyplot as plt
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import average_precision_score
from sklearn.metrics import accuracy_score
from sklearn.metrics import roc_curve, auc, confusion_matrix
from scipy import interp
from itertools import cycle

from image_recognition_util.classification_score_matrix import ClassificationScoreMatrix


def y_score_from_predictions(ordered_classes, predictions_per_image):
    """
    Create a dense array of the score per class for each image

    :param ordered_classes: the labels used in the classification. These are assumed to be in the same order as passed to other functions
    :param predictions_per_image: a sequence of [score0, ..., scoreN] sequences, one for each image. Each score-sequence must have the same  order as the ordered_classes
    :return: numpy.ndarray of shape (number of images, number of classes)
    Axis 0 indexes over images, axis 1 is the score for the class at that position
    y_score[x][y] is the score of image x for the class ordered_classes[y]
    """
    n_images = len(predictions_per_image)
    n_classes = len(ordered_classes)

    y_score = np.zeros(shape=(n_images, n_classes))

    for image_index, prediction_per_image in enumerate(predictions_per_image):
        prediction_dict = dict(zip(ordered_classes, prediction_per_image))
        for label_index, label in enumerate(ordered_classes):
            y_score[image_index][label_index] = prediction_dict[label]
    return y_score


def y_test_from_ground_truth(ordered_classes, ground_truth_per_image):
    """
    Create a sparse array of the one-hot ground truth classification per image

    :param ordered_classes: the labels used in the classification. These are assumed to be in the same order as passed to other functions
    :param ground_truth_per_image: a list of classes, one for each image
    :return: numpy.ndarray of shape (number of images, number of classes)
    Axis 0 indexes over images, axis 1 is the score for the class at that position
    (i.e. 1 because this takes the ground truth as input)
    y_test[x][y] is 1 when the ground truth class for image x is ordered_classes[y]. The other values are 0
    """
    n_images = len(predictions_per_image)
    n_classes = len(ordered_classes)

    y_test = np.zeros(shape=(n_images, n_classes))
    for image_index, ground_truth_class in enumerate(ground_truth_per_image):
        if ground_truth_class in ordered_classes:
            y_test[image_index][ordered_classes.index(ground_truth_class)] = 1  # Others are left at zero
        # else: pass, leave zero. We've already warned for this situation when loading the data.

    return y_test


def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=80)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black",
                 fontsize=6)

    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.grid()

parser = argparse.ArgumentParser(description='Calculate various metrics of a classifier that assigns a score per class')

parser.add_argument("input", type=str, help='Input csv file. First column is ground truth label, other columns are the '
                                            'per-class scores, as output by `evaluate_classifier`',
                    default="result.csv")

parser.add_argument('-o', '--output', type=str, help='Output image file',
                    required=False)

parser.add_argument('-t', '--title', type=str, help='Title for the graph',
                    default="",
                    required=False)

args = parser.parse_args()

classification_score_matrix = ClassificationScoreMatrix.from_file(args.input)

ordered_classes = classification_score_matrix.labels
y_true_labels = classification_score_matrix.classifications_ground_truth

predictions_per_image = classification_score_matrix.classifications_scores

n_classifications = len(y_true_labels)
print "Read {} prediction scores".format(n_classifications)

n_classes = len(ordered_classes)
y_predicted_score = y_score_from_predictions(ordered_classes, predictions_per_image)
y_true_sparse = y_test_from_ground_truth(ordered_classes, y_true_labels)

y_predicted_class_indices = y_predicted_score.argmax(axis=1)  # Find the index for each row that has the maximum value
y_predicted_classes = [ordered_classes[class_index] for class_index in y_predicted_class_indices]

# setup consistent colors+markers for each class
cmap = plt.get_cmap('gist_rainbow')
colors = [cmap(i) for i in np.linspace(0, 1, n_classes)]
markers = cycle([' ', 'o', 'x'])

class_colors = dict(zip(ordered_classes, colors))
class_markers = dict(zip(ordered_classes, markers))

accuracy = accuracy_score(y_true_labels, y_predicted_classes)
print "Accuracy: {acc:.3f}".format(acc=accuracy)

# From - http://scikit-learn.org/stable/modules/model_evaluation.html#precision-recall-f-measure-metrics
#      - http://scikit-learn.org/stable/modules/model_evaluation.html#accuracy-score
print 'Precision is the ability of the classifier not to label as positive a sample that is negative.'
print 'Recall is the ability of the classifier to find all the positive samples.'
print 'Accuracy is the fraction of correct predictions.'

# For each class
precision = dict()
recall = dict()
p_r_thresholds = dict()
average_precision = dict()

for i in range(n_classes):
    precision[i], recall[i], p_r_thresholds[i] = precision_recall_curve(y_true_sparse[:, i],
                                                                        y_predicted_score[:, i])
    average_precision[i] = average_precision_score(y_true_sparse[:, i], y_predicted_score[:, i])

# A "micro-average": quantifying score on all classes jointly
precision["micro"], recall["micro"], _ = precision_recall_curve(y_true_sparse.ravel(),
                                                                y_predicted_score.ravel())
average_precision["micro"] = average_precision_score(y_true_sparse, y_predicted_score,
                                                     average="micro")
print('Average precision score, micro-averaged over all classes: {0:0.3f}'
      .format(average_precision["micro"]))

plt.figure(figsize=(25, 20))
title = args.title + """\n""" if args.title else ""
title += """Classifier tested on {ni} images, {nc} classes. avg. Precision: {prec:.3f}, Accuracy {acc:.3f}"""\
    .format(ni=n_classifications, nc=n_classes, prec=average_precision["micro"], acc=accuracy)
plt.suptitle(title, fontsize=16)
plt.rc("axes", labelsize=15)

lw = 2

# Plot all Threshold vs precision and recall curves
plt.subplot(2, 2, 1)
for i in range(n_classes):
    cls = ordered_classes[i]
    lowest_prob = np.min(y_predicted_score[:, 0])

    # Make the p_r_threshold the same shape as precision by including the lowest prob
    inclusive_thresh = np.insert(p_r_thresholds[i], 0, lowest_prob)

    plt.plot(inclusive_thresh,
             precision[i],
             color=class_colors[cls], marker=class_markers[cls],
             lw=lw,
             label='Precision {}'.format(ordered_classes[i]))

    plt.plot(inclusive_thresh,
             recall[i],
             color=class_colors[cls], marker=class_markers[cls],
             lw=lw,
             linestyle='dashed',
             label='Recall {}'.format(ordered_classes[i]))
plt.xlabel("Threshold ($t$)")
plt.ylabel(r'Precision $\frac{T_p(t)}{T_p(t) + F_p(t)}$ & Recall ($\frac{T_p(t)}{T_p(t) + F_n(t)})$')
plt.legend(loc="best", prop={'size': 10}, ncol=3)

plt.subplot(2, 2, 2)

f_scores = np.linspace(0.2, 0.8, num=4)
lines = []
labels = []
for f_score in f_scores:
    x = np.linspace(0.01, 1)
    y = f_score * x / (2 * x - f_score)
    l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)
    plt.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))

lines.append(l)

labels.append('iso-f1 curves')
l, = plt.plot(recall["micro"], precision["micro"], color='black', lw=2)
lines.append(l)

plt.step(recall['micro'], precision['micro'], color='b', alpha=0.2,
         where='post')
plt.fill_between(recall["micro"], precision["micro"], step='post', alpha=0.2,
                 color='b')
labels.append('micro-average (area = {0:0.2f})'
              ''.format(average_precision["micro"]))

for i in range(n_classes):
    cls = ordered_classes[i]
    l, = plt.plot(recall[i], precision[i], lw=2, color=class_colors[cls], marker=class_markers[cls])
    lines.append(l)
    labels.append('"{0}" (area = {1:0.2f})'
                  ''.format(cls, average_precision[i]))

fig = plt.gcf()
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel(r'Recall ($\frac{T_p(t)}{T_p(t) + F_n(t)})$')
plt.ylabel(r'Precision $\frac{T_p(t)}{T_p(t) + F_p(t)}$')
plt.title('Multi-class Precision-Recall curves')
plt.legend(lines, labels, loc=(0, 0), prop=dict(size=10), ncol=3)

# Compute ROC curve and ROC area for each class
fpr = dict()
tpr = dict()
roc_thresholds = dict()
roc_auc = dict()
for i in range(n_classes):
    fpr[i], tpr[i], roc_thresholds[i] = roc_curve(y_true_sparse[:, i], y_predicted_score[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Compute micro-average ROC curve and ROC area
fpr["micro"], tpr["micro"], _ = roc_curve(y_true_sparse.ravel(), y_predicted_score.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])
# Compute macro-average ROC curve and ROC area

# First aggregate all false positive rates
all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))

# Then interpolate all ROC curves at this points
mean_tpr = np.zeros_like(all_fpr)
for i in range(n_classes):
    mean_tpr += interp(all_fpr, fpr[i], tpr[i])

# Finally average it and compute AUC
mean_tpr /= n_classes

fpr["macro"] = all_fpr
tpr["macro"] = mean_tpr
roc_auc["macro"] = auc(fpr["macro"], tpr["macro"])

# Plot threshold vs true and false positive rate
plt.subplot(2, 2, 3)

for i in range(n_classes):
    cls = ordered_classes[i]
    plt.plot(roc_thresholds[i],
             tpr[i],
             color=class_colors[cls], marker=class_markers[cls],
             lw=lw,
             label='Tpr {}'.format(ordered_classes[i]))

    plt.plot(roc_thresholds[i],
             fpr[i],
             color=class_colors[cls], marker=class_markers[cls],
             lw=lw,
             linestyle='dashed',
             label='Fpr {}'.format(ordered_classes[i]))

plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.ylabel(r'True  $\frac{T_p(t)}{T_p(t) + F_p(t)}$ & False $\frac{F_p(t)}{F_p(t) + T_n(t)}$ Positive Rate: ')
plt.xlabel('Threshold ($t$)')
plt.title('Threshold vs. True & False positive rate')

# Compute confusion matrix
cnf_matrix = confusion_matrix(y_true_labels, y_predicted_classes)

# Plot normalized confusion matrix
plt.subplot(2, 2, 4)
plot_confusion_matrix(cnf_matrix, classes=ordered_classes, normalize=True,
                      title='Normalized confusion matrix')

if args.output:
    plt.savefig(args.output)
else:
    plt.show()
